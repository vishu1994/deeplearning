{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38829b68",
   "metadata": {},
   "source": [
    "- In **information theory**, we like to describe the “surprise” of an event. An event is more surprising the less likely it is, meaning it contains more information.\n",
    "    - Low Probability Event (surprising): More information.\n",
    "    - Higher Probability Event (unsurprising): Less information.\n",
    "- **Information(I)** is defined as number of bits required to transmit a message x which is given by $log_{2}(p(x))$.\n",
    "    - Information is low for events which are certain to happen , as such events have little to no surprise \n",
    "i.e I(x) = 0 , when p(x) = 1 .\n",
    "    - I is infinite for events which are certain to not happen i.e when p(x) = 0 . In practical terms, this means that if you have an event with a probability of 0, you can't express how surprising or informative it would be in terms of bits of information because it's beyond the scale of finite information. Such events are often referred to as \"impossible events\" in probability theory.\n",
    "- **Entropy** is defined as the average(Weighted Average) number of bits required to transmit an event/Message drawn randomly from a Random variable X. \n",
    "    - A skewed probability distribution has less “surprise” and in turn a low entropy because likely events dominate. Balanced distribution are more surprising and turn have higher entropy because events are equally likely.\n",
    "\n",
    "        - **Skewed Probability Distribution** (unsurprising): Low entropy.  E.g - Sun Rises in the East, p(East)=1 .\n",
    "        - **Balanced Probability Distribution** (surprising): High entropy. E.g - Flipping a coin, p(head)=p(Tail)=0.5\n",
    "    - Let X be a random variable which has n possible events $x_1, x_2,x_3 .... x_n$ . Then Entropy is define as below<br>\n",
    "    $H(x) = - \\sum _{i=1}^{n} p(x_i)*log_{2}(p(x_i))$\n",
    "- **Cross Entropy**\n",
    "    - Cross entropy quantifies the difference between two probability distributions by measuring the average number of bits needed to encode events from one distribution using an optimal code based on another distribution. In essence, it calculates how inefficiently you would encode events from the true distribution if you used the predicted distribution for encoding.\n",
    "    - The cross-entropy between two probability distributions, such as Q from P, can be stated formally as H(P, Q), Where H() is the cross-entropy function, P may be the target distribution and Q is the approximation of the target distribution. <br>\n",
    "        $H(P,Q) = - \\sum _{i=1}^{n} P(x_i)*log_{2}(Q(x_i))$ <br> Where $P(x_i)$ is the probability of the event $x_i$ in P, $Q(x_i)$ is the probability of event $x_i$ in Q\n",
    "    - If the two distributions are identical, the cross entropy is minimized and equal to the entropy of the true distribution. In this case, there's no inefficiency in encoding.\n",
    "    - As the predicted distribution diverges from the true distribution, the cross entropy increases, indicating a higher inefficiency in encoding.\n",
    "- Cross entropy measures the dissimilarity between two probability distributions and is often used as a loss function in machine learning.\n",
    "- **KL divergence** measures the relative entropy between two distributions, quantifying the information lost when approximating one with the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c79c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:case_solving] *",
   "language": "python",
   "name": "conda-env-case_solving-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
